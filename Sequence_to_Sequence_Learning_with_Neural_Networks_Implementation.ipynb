{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sequence to Sequence Learning with Neural Networks - Implementation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOsVcpmwGaEHjCplJkz6wiN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EliaTorre/NLP/blob/main/Sequence_to_Sequence_Learning_with_Neural_Networks_Implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sequence to Sequence Learning with Neural Networks"
      ],
      "metadata": {
        "id": "SjpS5csONR6W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "IUt32A0RNAZN"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import math\n",
        "import time\n",
        "\n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchtext.legacy.datasets import Multi30k\n",
        "from torchtext.legacy.data import Field, BucketIterator\n",
        "\n",
        "from torchtext.data.metrics import bleu_score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading Spacy packages "
      ],
      "metadata": {
        "id": "oee3FIgbTVMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!python -m spacy download en\n",
        "#!python -m spacy download de"
      ],
      "metadata": {
        "id": "CtMPPUqpT1jB"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initializing the seed to enforce repdocuibility of results"
      ],
      "metadata": {
        "id": "u0dn8WnmTaMT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 4321\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "id": "mDz00YGFNf_i"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the spacy modules for German and English"
      ],
      "metadata": {
        "id": "D3Z4hzYdTmt4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spacy_de = spacy.load('de_core_news_sm')\n",
        "spacy_en = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "OKurajpsNp8q"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the tokenizer functions, following the paper, I reverted the order of the tokens within the german sentences"
      ],
      "metadata": {
        "id": "S9EqHQlQTsev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def deutsch_tokenizer(text):\n",
        "    return [token.text for token in spacy_de.tokenizer(text)][::-1]\n",
        "\n",
        "def english_tokenizer(text):\n",
        "    return [token.text for token in spacy_en.tokenizer(text)]"
      ],
      "metadata": {
        "id": "C1Jy3jm-NtJZ"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Through pytorch's Field function I appended \"sos\" and \"eos\" tokens at the beginning and end of the sentences and transformed all of the sentences' words to lowercase"
      ],
      "metadata": {
        "id": "NDHZC7mcU8Sn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DE = Field(tokenize = deutsch_tokenizer, init_token = '<sos>', eos_token = '<eos>', lower = True)\n",
        "EN = Field(tokenize = english_tokenizer, init_token = '<sos>', eos_token = '<eos>', lower = True)"
      ],
      "metadata": {
        "id": "iDOM18puN1Jd"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I downloaded the Multi30K dataset which contains the parallel german-english-french tranlsation of approx. 30k sentences with approx. 12 words per sentence each. I used torchtext.datasets split attribute to divide the dataset in train/validation/test, where \"exts\" attribute specifies which language to use as source and which to use as target"
      ],
      "metadata": {
        "id": "csCx0s4DV-8W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, validation_data, test_data = Multi30k.splits(exts = ('.de', '.en'), fields = (DE, EN))"
      ],
      "metadata": {
        "id": "SCm0bsqLN_XU"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"# of training instances: {len(train_data.examples)}\")\n",
        "print(f\"# of validation instances: {len(validation_data.examples)}\")\n",
        "print(f\"# of testing instances: {len(test_data.examples)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rbwt9-yOHHO",
        "outputId": "222506de-a910-412f-f6a0-69f958c96df7"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# of training instances: 29000\n",
            "# of validation instances: 1014\n",
            "# of testing instances: 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I proceed in building the german (DE) and english (EN) vocabularies from the training data enforcing that only words which appear at least twice are included, otherwise an \"UNK\" token is put in their place"
      ],
      "metadata": {
        "id": "Y2Ka1IHBYy-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DE.build_vocab(train_data, min_freq = 2)\n",
        "EN.build_vocab(train_data, min_freq = 2)"
      ],
      "metadata": {
        "id": "5K2w9mLMOPN2"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"# of tokens in the German vocabulary: {len(DE.vocab)}\")\n",
        "print(f\"# of tokens in the English vocabulary: {len(EN.vocab)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvH09EjoOS-n",
        "outputId": "7b6f79f7-075b-4dc4-cd59-1933ef09ed9a"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# of tokens in the German vocabulary: 7855\n",
            "# of tokens in the English vocabulary: 5893\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I defined the device such that GPU can be exploited to speed up the training"
      ],
      "metadata": {
        "id": "-08RWij4ZeOd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "vBOSM_KiPmZk"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I define the iterators with a batch of 128, such that the data is transformed to an iterable object with a source and target attribute that maps tokenized words to their index in the vocabulary. \n",
        "\n",
        "I used \"BucketIterator\" instead of the standard \"Iterator\" because it creates batches that minimize the padding within sentences, such that it speeds up computation"
      ],
      "metadata": {
        "id": "YGHpjgagZsXj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch = 128\n",
        "train_iterator, validation_iterator, test_iterator = BucketIterator.splits((train_data, validation_data, test_data), batch_size = batch, device = device)"
      ],
      "metadata": {
        "id": "YLgnZprtPnHm"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following the paper I developed an encoder-decoder architecture which is comprised in three main classes: Encoder, Decoder and Seq2Seq. \n",
        "\n",
        "The Encoder is: \"A multilayered Long-Short-Term Memory (LSTM) that maps the input sequence to a vector of fixed dimensionality [...] the LSTM is known to learn problems with long range temporal dependencies, so an LSTM may succeed in this setting\". \n",
        "\n",
        "The Decoder is: \"The second LSTM is essentially a recurrent neural network language model except that it is conditioned on the input sequence\". It decodes the target sequence from the vector received by the encoder. \n",
        "\n",
        "Finally the Seq2Seq class has the purpose of including both the Encoder and the Decoder to perform the entire process of encoding-decoding\n"
      ],
      "metadata": {
        "id": "Y1meLkfTcNKM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Encoder class takes 5 inputs: \n",
        "- \"input_dim\", i.e., the dimensionality of the source vocabulary.\n",
        "- \"embedding_dim\", i.e., the size of the embedding layer.\n",
        "- \"hidden_dim\", i.e., the size of the hidden and cell states.\n",
        "- \"n_layers\", i.e., the number of layers in the LSTM.\n",
        "- \"dropout\", i.e., the share of dropout in our model.\n",
        "\n",
        "For what concerns the depth of the RNN, as opposed to the 4-layers of the paper architeture, I developed a 2-layer LSTM to maintain computational time reasonable. \n",
        "\n",
        "Then I defined the \"forward\" method of the class, where the input sentence is embedded and dropout is performed. Finally, self.rnn performs the calculation of the hidden states and three outputs are created: \n",
        "\n",
        "- \"outputs\", i.e., a list of the hidden states at each time step. \n",
        "- \"hidden\", i.e., the two final hidden states of the two layers. \n",
        "- \"cell\", i.e., a list of the cell state at each time step.\n",
        "\n",
        "I return just \"hidden\" and \"cell\" as \"outputs\" is not needed in this implementation. "
      ],
      "metadata": {
        "id": "qbWuUglGfmst"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
        "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout = dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, de):  \n",
        "        embedded = self.dropout(self.embedding(de))    \n",
        "        outputs, (hidden, cell) = self.rnn(embedded)\n",
        "          \n",
        "        return hidden, cell"
      ],
      "metadata": {
        "id": "6sDk3MSZP-_Z"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The inputs of the Decoder are the same of the Encoder except for \"output_dim\", i.e., the size of the target vocabulary.\n",
        "\n",
        "The \"forward\" method of the Decoder is similar to the one of the Encoder however we now have to \"unsqueeze\" the input sequence to perform the decoding one token at a time. Then I performed embedding, dropout and the calculation of hidden and cell states. Finally, I passed the output through the linear layer to obtain the prediction."
      ],
      "metadata": {
        "id": "snS2lWJdlVqu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, embedding_dim, hidden_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        self.output_dim = output_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.embedding = nn.Embedding(output_dim, embedding_dim)\n",
        "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout = dropout)\n",
        "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, input, hidden, cell):\n",
        "        input = input.unsqueeze(0)        \n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
        "        prediction = self.fc_out(output.squeeze(0))\n",
        "        \n",
        "        return prediction, hidden, cell"
      ],
      "metadata": {
        "id": "EKXduUQuQShV"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following \"Seq2Seq\" class aims at receiving the source sequence, evaluate the context vectors through the \"Encoder\" class and predict the target sentence through the \"Decoder\" class. \n",
        "\n",
        "In particular, \"Seq2Seq\" receives the encoder and decoder as inputs. \n",
        "Then, in the \"forward\" method, I initialize some dimensionality variables (\"batch_size, \"en_len\", \"en_vocab\") to create a torch.zeros vector with these dimensions. I get the hidden and cell states by performing the encoding of the german sequences.\n",
        "\n",
        "Then a loop is performed, where at each iteration the prediction of a single token is performed by the decoder."
      ],
      "metadata": {
        "id": "oJHa_T--nfkZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        \n",
        "    def forward(self, de, en, ratio = 0.5):         \n",
        "        batch_size = en.shape[1]\n",
        "        en_len = en.shape[0]\n",
        "        en_vocab = self.decoder.output_dim\n",
        "        outputs = torch.zeros(en_len, batch_size, en_vocab).to(self.device)\n",
        "        hidden, cell = self.encoder(de)\n",
        "        input = en[0,:]\n",
        "        for t in range(1, en_len):\n",
        "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
        "            outputs[t] = output\n",
        "            force = random.random()<ratio\n",
        "            best = output.argmax(1) \n",
        "            input = en[t] if force else best\n",
        "        \n",
        "        return outputs"
      ],
      "metadata": {
        "id": "Vbhm7XfqQlvq"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here I initialize the parameters of the model"
      ],
      "metadata": {
        "id": "0Am9yvUoq0W-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim, output_dim = len(DE.vocab), len(EN.vocab)\n",
        "encoder_embedding_dim, decoder_embedding_dim = 256, 256\n",
        "hidden_dim, n_layers = 512, 2\n",
        "encoder_dropout, decoder_dropout = 0.5, 0.5\n",
        "\n",
        "encoder = Encoder(input_dim, encoder_embedding_dim, hidden_dim, n_layers, encoder_dropout)\n",
        "decoder = Decoder(output_dim, decoder_embedding_dim, hidden_dim, n_layers, decoder_dropout)\n",
        "\n",
        "model = Seq2Seq(encoder, decoder, device).to(device)"
      ],
      "metadata": {
        "id": "evNJSK5YRJLa"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following the paper, I performed the initialization of the model weights according to a uniform distribution on the interval (-0.08, 0.08). And I apply that to the model"
      ],
      "metadata": {
        "id": "cQ9qK_9mq5fy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def weights(model):\n",
        "    for n, p in model.named_parameters():\n",
        "        nn.init.uniform_(p.data, -0.08, 0.08)\n",
        "model.apply(weights);"
      ],
      "metadata": {
        "id": "A_QO10O6RtoZ"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'The model has {parameters(model):,} trainable parameters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVGgeImqR-Qh",
        "outputId": "17e90467-95dc-4dbc-b20f-b29ecab2f3a6"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 13,899,013 trainable parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here I defined the optimizer, i.e., \"ADAM\", and the loss function, i.e., \"CrossEntropyLoss\". In particular, by setting the \"ignore_index\" parameter to \"EN_PAD_IDX\", it calculate the avg. loss per token ignoring the loss on \"pad\" tokens"
      ],
      "metadata": {
        "id": "V6JZEnHAx4YT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(model.parameters())\n",
        "en_pad_idx = EN.vocab.stoi[EN.pad_token]\n",
        "loss_function = nn.CrossEntropyLoss(ignore_index = en_pad_idx)"
      ],
      "metadata": {
        "id": "E0MTBf4RSCmZ"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, I define the functions \"train\" and \"test\". \n",
        "\n",
        "In the first one, I am setting \"model.train()\" such that dropout layer is considered when running the model. It iterates over the data iterator to update the parameters of the model: \n",
        "- Gets the german and english sentences.\n",
        "- Set to zero the gradients calculated previously.\n",
        "- Runs the model to obtain the predictions. \n",
        "- It reshapes input and output through .view() to fit the loss function. \n",
        "- Performs backpropagation and clips the gradient to avoind exploding gradients problems.\n",
        "- Updates the parameters through optimizer.step() \n",
        "- Updates epoch_loss\n",
        "\n",
        "Then it returns the epoch_loss averaged over all batches. "
      ],
      "metadata": {
        "id": "tErt-5-ZyjA0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, iterator, optimizer, loss_function, clip):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for i, batch in enumerate(iterator):\n",
        "        de = batch.src\n",
        "        en = batch.trg\n",
        "        optimizer.zero_grad()\n",
        "        output = model(de, en)\n",
        "        output_dim = output.shape[-1]\n",
        "        output = output[1:].view(-1, output_dim)\n",
        "        en = en[1:].view(-1)        \n",
        "        loss = loss_function(output, en)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ],
      "metadata": {
        "id": "ns_4f-JRSJCh"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The \"test\" function works similarly to the \"train\" function, however in this case, I set model.eval() such that no parameter optimization and dropout is performed. \n",
        "\n",
        "Furthermore, torch.no_grad() is introduced to avoid the computational cost of performing the gradients calculation."
      ],
      "metadata": {
        "id": "NX4ZTbdC2QEj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, iterator, loss_function):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(iterator):\n",
        "            de = batch.src\n",
        "            en = batch.trg\n",
        "            output = model(de, en, 0) \n",
        "            output_dim = output.shape[-1]\n",
        "            output = output[1:].view(-1, output_dim)\n",
        "            en = en[1:].view(-1)\n",
        "            loss = loss_function(output, en)\n",
        "            epoch_loss += loss.item()\n",
        "            \n",
        "    return epoch_loss / len(iterator)"
      ],
      "metadata": {
        "id": "K1FT7JhdSfw_"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following \"translate\" function has the purpose of generating a translation of the german text and it is needed to evaluate the model according to the BLEU score."
      ],
      "metadata": {
        "id": "-sOrlQTo3TSF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(sentence, de_field, en_field, model, device, max = 50):\n",
        "    model.eval()\n",
        "    tokens = [token.lower() for token in sentence]\n",
        "    tokens = [de_field.init_token] + tokens + [de_field.eos_token]\n",
        "    de_idx = [de_field.vocab.stoi[token] for token in tokens]\n",
        "    de_tensor = torch.LongTensor(de_idx).unsqueeze(1).to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      hidden, cell = model.encoder(de_tensor)\n",
        "    \n",
        "    en_idx = [en_field.vocab.stoi[en_field.init_token]]\n",
        "    \n",
        "    for i in range(max):\n",
        "        en_tensor = torch.LongTensor([en_idx[-1]]).to(device)\n",
        "        with torch.no_grad():\n",
        "            output, hidden, cell = model.decoder(en_tensor, hidden, cell)\n",
        "        pred_token = output.argmax(1).item()\n",
        "        en_idx.append(pred_token)\n",
        "        if pred_token == en_field.vocab.stoi[en_field.eos_token]:\n",
        "            break\n",
        "\n",
        "    en_tokens = [en_field.vocab.itos[i] for i in en_idx]\n",
        "\n",
        "    return en_tokens[1:]"
      ],
      "metadata": {
        "id": "TD_eis7BTOQh"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The \"bleu\" function allows the computation of the BLEU score, which is a a metric specifically designed to assess the quality of a translation. It checks the overlapping between the actual and predicted english sequences in terms of their n-grams and outputs a score between 0 and a 100 (with a 100 being a perfect translation)"
      ],
      "metadata": {
        "id": "mnwZ8Gzt3r8H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bleu(data, de_field, en_field, model, device, max = 50):\n",
        "    ens = []\n",
        "    pred_ens = []  \n",
        "\n",
        "    for x in data:\n",
        "        de = vars(x)['src']\n",
        "        en = vars(x)['trg']\n",
        "        pred_en = translate(de, de_field, en_field, model, device, max)\n",
        "        pred_en = pred_en[:-1]\n",
        "        pred_ens.append(pred_en)\n",
        "        ens.append([en])\n",
        "        \n",
        "    return bleu_score(pred_ens, ens)"
      ],
      "metadata": {
        "id": "PGPfieyNTs2Y"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here I defined a function to measure the time elapsed between each epoch of the training process"
      ],
      "metadata": {
        "id": "32KGJkzx5wIy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_time(start, end):\n",
        "    elapsed = end - start\n",
        "    mins = int(elapsed / 60)\n",
        "    secs = int(elapsed - (mins * 60))\n",
        "    return mins, secs"
      ],
      "metadata": {
        "id": "0BlJUS-QSu8u"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, I defined the training iteration process of the architecture which evaluates the \"train_loss\" and \"validation_loss\" of each epoch and keeps track of the best parameters configuration according to the validation_loss. \n",
        "\n",
        "It then prints a summary of train/validation loss of each epoch and the correspondent perplexity metric"
      ],
      "metadata": {
        "id": "XeZIqcf652Oi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs, clip, best = 15, 1, float('inf')\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "    train_loss = train(model, train_iterator, optimizer, loss_function, clip)\n",
        "    validation_loss = test(model, validation_iterator, loss_function)\n",
        "    end = time.time()\n",
        "    mins, secs = count_time(start, end)\n",
        "\n",
        "    if validation_loss < best:\n",
        "        best = validation_loss\n",
        "        torch.save(model.state_dict(), 'LSTM-model.pt')\n",
        "\n",
        "    print(f'Epoch: {epoch + 1}, Time: {mins}m {secs}s')\n",
        "    print(f'Train Loss: {train_loss:.3f}, Validation Loss: {validation_loss:.3f} ')\n",
        "    print(f'Train PPL: {math.exp(train_loss):7.3f}, Validation PPL: {math.exp(validation_loss):7.3f}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TeOQKaxZSxQo",
        "outputId": "a798a6ec-6f55-4883-fe88-0ef40a78b8ad"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Time: 0m 29s\n",
            "Train Loss: 5.058, Validation Loss: 4.936 \n",
            "Train PPL: 157.267, Validation PPL: 139.174\n",
            "\n",
            "Epoch: 2, Time: 0m 29s\n",
            "Train Loss: 4.506, Validation Loss: 4.813 \n",
            "Train PPL:  90.584, Validation PPL: 123.145\n",
            "\n",
            "Epoch: 3, Time: 0m 29s\n",
            "Train Loss: 4.173, Validation Loss: 4.684 \n",
            "Train PPL:  64.883, Validation PPL: 108.189\n",
            "\n",
            "Epoch: 4, Time: 0m 29s\n",
            "Train Loss: 3.979, Validation Loss: 4.457 \n",
            "Train PPL:  53.465, Validation PPL:  86.233\n",
            "\n",
            "Epoch: 5, Time: 0m 29s\n",
            "Train Loss: 3.785, Validation Loss: 4.370 \n",
            "Train PPL:  44.040, Validation PPL:  79.061\n",
            "\n",
            "Epoch: 6, Time: 0m 29s\n",
            "Train Loss: 3.637, Validation Loss: 4.248 \n",
            "Train PPL:  37.979, Validation PPL:  69.989\n",
            "\n",
            "Epoch: 7, Time: 0m 29s\n",
            "Train Loss: 3.513, Validation Loss: 4.129 \n",
            "Train PPL:  33.533, Validation PPL:  62.094\n",
            "\n",
            "Epoch: 8, Time: 0m 29s\n",
            "Train Loss: 3.369, Validation Loss: 4.107 \n",
            "Train PPL:  29.046, Validation PPL:  60.790\n",
            "\n",
            "Epoch: 9, Time: 0m 30s\n",
            "Train Loss: 3.264, Validation Loss: 4.070 \n",
            "Train PPL:  26.142, Validation PPL:  58.569\n",
            "\n",
            "Epoch: 10, Time: 0m 29s\n",
            "Train Loss: 3.147, Validation Loss: 3.949 \n",
            "Train PPL:  23.255, Validation PPL:  51.860\n",
            "\n",
            "Epoch: 11, Time: 0m 29s\n",
            "Train Loss: 3.054, Validation Loss: 3.877 \n",
            "Train PPL:  21.198, Validation PPL:  48.257\n",
            "\n",
            "Epoch: 12, Time: 0m 29s\n",
            "Train Loss: 2.941, Validation Loss: 3.787 \n",
            "Train PPL:  18.931, Validation PPL:  44.110\n",
            "\n",
            "Epoch: 13, Time: 0m 30s\n",
            "Train Loss: 2.848, Validation Loss: 3.740 \n",
            "Train PPL:  17.255, Validation PPL:  42.084\n",
            "\n",
            "Epoch: 14, Time: 0m 29s\n",
            "Train Loss: 2.749, Validation Loss: 3.777 \n",
            "Train PPL:  15.626, Validation PPL:  43.701\n",
            "\n",
            "Epoch: 15, Time: 0m 29s\n",
            "Train Loss: 2.701, Validation Loss: 3.705 \n",
            "Train PPL:  14.901, Validation PPL:  40.637\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following cells, I evaluate the performance of the trained architecture on an unseen sample test and compute its BLEU score"
      ],
      "metadata": {
        "id": "VN5eXN1S6gWM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('LSTM-model.pt'))\n",
        "test_loss = test(model, test_iterator, loss_function)\n",
        "print(f'Test Loss: {test_loss:.3f}, Test PPL: {math.exp(test_loss):7.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ike2ta-MTJps",
        "outputId": "cdee526b-102d-4213-c27c-c9a70a6a2e90"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 3.710, Test PPL:  40.861\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bleu_score = bleu(test_data, DE, EN, model, device)\n",
        "print(f'BLEU score: {bleu_score*100:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0waBRf4oULV-",
        "outputId": "dcf045d6-2671-4044-d921-c4de9a2828f4"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU score: 14.14\n"
          ]
        }
      ]
    }
  ]
}